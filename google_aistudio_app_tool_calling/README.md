# Google AI Studio App - Tool Calling Demo (Refactored)

Este proyecto ha sido refactorizado para utilizar una arquitectura **Frontend - Backend**, migrando la lÃ³gica de herramientas de IA a un backend con Python y Llama 3.2.

## ğŸ—ï¸ Nueva Estructura

- **`frontend/`**: AplicaciÃ³n React (Vite) original.
- **`backend/`**: Servidor FastAPI con Python.

## ğŸ› ï¸ Requisitos

- Python 3.8+
- Node.js 16+
- Ollama corriendo localmente con el modelo `llama3.2`.

## ğŸš€ CÃ³mo Ejecutar

### 1. Iniciar el Backend (Python)

```bash
cd backend
pip install -r requirements.txt
python main.py
```
*El servidor iniciarÃ¡ en http://localhost:8000*

### 2. Iniciar el Frontend (React)

En una nueva terminal:

```bash
cd frontend
npm install
npm run dev
```
*La aplicaciÃ³n abrirÃ¡ en http://localhost:5173*

## ğŸ§  LÃ³gica de AI y Tools

La lÃ³gica que anteriormente residÃ­a en `geminiService.ts` ha sido migrada a `backend/main.py`.

- **Endpoint `/chat`**: Recibe mensajes del usuario y consulta a Ollama.
- **Tool `consultar_reportes`**: Implementada en Python, consulta el estado de reportes en memoria (`REPORTS_DB`).
- **Endpoint `/upload`**: Maneja la carga y validaciÃ³n de archivos, actualizando el estado de los reportes en el backend.

## ğŸ“ Notas sobre la SimulaciÃ³n

- La base de datos de reportes es **en memoria** en el backend. Se reinicia si detienes el proceso de Python.
- Al iniciar, el backend genera reportes "PENDING" para la fecha actual.
- Puedes usar las plantillas del frontend para generar archivos vÃ¡lidos y probar la carga.