# Retail AI Agent - Local LLM Tool Calling

Este proyecto implementa un agente de IA para Retail utilizando **Llama 3.2** (vÃ­a Ollama) y **Python**. Demuestra el patrÃ³n de arquitectura **Tool Calling** (Function Calling), donde el LLM actÃºa como orquestador de lÃ³gica de negocio.

## ğŸ“¸ Evidencias
- [Ver ejemplos de uso V1](./evidencias_v1.md)
- [Ver ejemplos de uso V2](./evidencias_v2.md)

## ğŸ“š Conceptos Clave

Para una explicaciÃ³n detallada de cÃ³mo funciona el Tool Calling desde la perspectiva de un desarrollador de software tradicional (Java/.NET), consulta:
ğŸ‘‰ **[CONCEPTOS_PARA_DEVS](./CONCEPTOS_TOOL_CALLING.md)**

## ğŸ› ï¸ TecnologÃ­as Utilizadas

### ğŸ’» Entorno de Desarrollo IDE
  - **Antigravity**
  - **Modelo** Gemini 3 Pro(High)

### Lenguaje de ProgramaciÃ³n
- **Python 3.x**

### ğŸ§  Inteligencia Artificial
- **Modelo**: Llama 3.2 (ejecutado localmente vÃ­a Ollama)

## ğŸ› ï¸ InstalaciÃ³n y EjecuciÃ³n

### ğŸš€ Requisitos

-   **Python 3.8+**
-   **Ollama** instalado y ejecutÃ¡ndose
-   **Modelo llama3.2** descargado en Ollama

### 1. Instalar y Configurar Ollama

Si aÃºn no tienes Ollama instalado:

**Windows/macOS/Linux:**
```bash
# Descarga desde https://ollama.com/download
# O usa el instalador apropiado para tu sistema
```
**Descargar el modelo Llama 3.2:**
```bash
ollama pull llama3.2
```

**Iniciar el servidor Ollama:**
```bash
ollama serve
```

El servidor Ollama debe estar ejecutÃ¡ndose en `http://localhost:11434` (puerto por defecto).

### 2. Instalar Dependencias de Python

Navega a la carpeta del proyecto:
```bash
cd llm_tool_calling
```

(Opcional) Crea y activa un entorno virtual:
```bash
python -m venv venv

# En Windows:
.\venv\Scripts\activate

# En macOS/Linux:
source venv/bin/activate
```

Insta las dependencias:
```bash
pip install -r requirements.txt
```

### 3. Ejecutar la AplicaciÃ³n

```bash
python main.py
```

## ğŸ§ª Casos de Uso Implementados

1.  **Consulta de Stock**: "Necesito verificar el stock de la laptop-001". (Invoca `InventoryService`).
2.  **Consulta de Atributos**: "Â¿CuÃ¡l es el precio del phone-002?". (Invoca `InventoryService`).
3.  **Pregunta General**: "Â¿QuÃ© opinas del retail?". (Respuesta directa del LLM, sin herramientas).

## ğŸ” Observaciones TÃ©cnicas

### Comportamiento del LLM
El modelo Llama 3.2 demostrÃ³:
- Excelente capacidad para identificar cuÃ¡ndo necesita informaciÃ³n externa
- Correcta extracciÃ³n de parÃ¡metros de las preguntas del usuario
- Buena integraciÃ³n de los resultados de las tools en respuestas naturales
- Capacidad de distinguir entre preguntas que requieren tools y las que no

### Ventajas del Approach
1. **SeparaciÃ³n de responsabilidades**: El LLM decide cuÃ¡ndo llamar a tools
2. **Flexibilidad**: FÃ¡cil agregar nuevas tools al sistema
3. **Transparencia**: El sistema muestra claramente cuÃ¡ndo se llama a una tool
4. **Escalabilidad**: El patrÃ³n se puede extender a mÃºltiples tools

## ğŸ¯ Conclusiones

El proyecto demuestra exitosamente:
1. ImplementaciÃ³n de function calling con Llama 3.2 (Ollama)
2. CreaciÃ³n de una tool personalizada para la vertical de Retail
3. Manejo correcto de casos que requieren y no requieren tools
4. IntegraciÃ³n fluida entre el LLM y funciones Python